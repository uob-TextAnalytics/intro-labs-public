{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 4: Regular Expressions and Text Normalisation\n",
    "\n",
    "### Learning Outcomes\n",
    "* Be able to set up a Python and Jupyter notebook environment for text analytics\n",
    "* Understand how to use regular expressions to preprocess text\n",
    "* Know how to carry out text normalisation using the NLTK library\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Getting started: how to set up your environment, Jupyter notebooks introduction\n",
    "* Acquiring raw text data\n",
    "* Regular expressions\n",
    "* Text normalisation \n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Setting up your environment\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all the packages you need for these labs. You can install either Anaconda or Miniconda, which will include the ```conda``` program. \n",
    "\n",
    "We provide a .yml file that lists all the packages you will need, and the versions that we have tested the labs with. You can use this file to create your environment as follows.\n",
    "\n",
    "1. Open a terminal. Use the command line to navigate to the directory containing this notebook and the file ```crossplatform_environment.yml```. You can use the command ```cd``` to change directory on the command line.\n",
    "\n",
    "1. Run the conda program by typing ```conda env create -f crossplatform_environment.yml```, then answer any questions that appear on the command line.\n",
    "\n",
    "1. Activate the environment by running the command ```conda activate data_analytics```.\n",
    "\n",
    "1. Make kernel available in Jupyter: ```python -m ipykernel install --user --name=data_analytics```.\n",
    "\n",
    "1. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` or ```jupyter notebook``` into your command line, depending on whether you prefer the full Jupyter lab development environment, or the simpler Jupyter notebook.\n",
    "\n",
    "1. Find this notebook and open it up again.\n",
    "\n",
    "1. Go to the top menu and change the kernel: click on 'Kernel'--> 'Change kernel' --> data_analytics.\n",
    "\n",
    "\n",
    "You should now be ready to go!\n",
    "\n",
    "The core libraries we will be using in this unit are:\n",
    "\n",
    "- [Datasets](https://huggingface.co/docs/datasets/), produced by HuggingFace, is a hub for lots of interesting text datasets.\n",
    "- [NLTK](https://www.nltk.org), a comprehensive NLP library.\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), for machine learning and classifier evaluation.\n",
    "- [Gensim](https://radimrehurek.com/gensim/), for topic modelling.\n",
    "\n",
    "The libraries above have good documentation, which is available either online (links above) or via Python itself, e.g. `help(numpy.array)` in the Python interpreter. \n",
    "\n",
    "### Refreshers for Python and Jupyter\n",
    "\n",
    "**Skip this part if you're already familiar with Python and Jupyter notebooks.**\n",
    "\n",
    "This lab assumes you have used Python and Jupyter Notebooks before. \n",
    "\n",
    "For an introduction or refresher on Python, see the [Introduction to Python lab](https://github.com/UoB-COMS21202/lab_sheets_public/tree/master/lab_1) or the University of Bristol [Beginning Python](https://milliams.gitlab.io/beginning_python/) course. If you are a beginner with Python, you might also like to look at Chapter 1 in the NLTK book, which also provides a guide for \"getting started with Python\": https://www.nltk.org/book/ \n",
    "\n",
    "You will need to use Python 3, not Python 2, and Python 3.6 or newer are recommended.\n",
    "\n",
    "The labs will be run on [Jupyter Notebook](http://jupyter.org/), an interactive coding environment embedded in a webpage supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels.  \n",
    "\n",
    "It allows you to enrich your code with complex comments formatted in Markdown and $\\LaTeX$, as well as to place the results of your computation right below your code.\n",
    "\n",
    "Notebooks are organised in cells which can contain either code (in our case, this will be Python code) or text, which can be easily and nicely formatted using the Markdown notation. \n",
    "\n",
    "To edit an already existing cell simply double-click on it. You can use the toolbar to insert new cells, edit and delete them (or use keyboard shortcuts which are very handy to speed up coding). \n",
    "\n",
    "Cells can be run, by hitting `shift+enter` when editing a cell or by clicking on the `Run` button at the top. Running a Markdown cell will simply display the formatted text, while running a code cell will execute the commands executed in it. \n",
    "\n",
    "**Note**: when you run a code cell, all the created variables, implemented functions and imported libraries will be then available to every other code cell. However, it is commonly assumed that cells will be run sequentially in terms of prerequisites. To reset all variables and functions (for debugging) simply click `Kernel > Restart` from the Jupyter menu.\n",
    "\n",
    "#### Markdown \n",
    "\n",
    "Markdown cells allow you to write fancy and simple comments: all of this is written in Markdown - double click on this cell to see the source. An introduction to Markdown syntax can be found [here](https://daringfireball.net/projects/markdown/syntax).\n",
    "\n",
    "As Markdown is translated to HTML upon displaying it also allows you to use pure HTML: more details are available [here](https://daringfireball.net/projects/markdown/syntax#html).\n",
    "\n",
    "Finally, you can also display simple $\\LaTeX$ equations in Markdown thanks to `MathJax` support. For inline equations wrap your equation between `$` symbols; for display mode equations use `$$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acquiring Raw Text Data\n",
    "\n",
    "Now, let's get some text data! We'll start with the Reuters dataset, which contains financial newswire articles.  Run the code below to download the data from [HuggingFace's datasets hub](https://huggingface.co/datasets/newsgroup):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"reuters21578\", \n",
    "    \"ModHayes\",  # Choose this variant of the dataset\n",
    "    split=\"train\",\n",
    "    cache_dir=\"./data_cache\",  # Save the data here \n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the documents in the dataset like elements in a list. For example, document 3 looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regular Expressions\n",
    "\n",
    "## 2.1 Search\n",
    "\n",
    "Now, let's get to grips with regular expressions. Suppose we want to discover all sentences discussing mergers between companies. A first step would be to find all occurrences of the word 'merger':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Python regular expressions library\n",
    "\n",
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall('merger', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has given us a list of matches in the variable `all_matches`, which all contain the string 'merger', but not the sentences themselves.\n",
    "This isn't very useful, but we can do better if we define the right regular expression!\n",
    "\n",
    "Regular expressions define a pattern, rather than a specific string, allowing us to generalise our search and retrieve a many different strings that match the pattern.\n",
    "In Python, we differentiate a regular expression from a normal string by putting an 'r' character in front of the string.\n",
    "\n",
    "We can generalise our search by using a _disjunction_, which will match against any one of a set of characters. The disjunction is written inside square brackets. \n",
    "\n",
    "Let's try to retrieve instances of the word \"merger\" followed by any letter. We can write a disjunction that matches any lower case letter as `[a-z]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall(r'merger [a-z]', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current search only matches a single letter of the word after 'merger'. The length of that following word is variable, so how can we write an expression to match the whole word? \n",
    "\n",
    "Here, we can use a special character, '\\*', which will match against zero or more repetitions of the preceding regular expression. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    matches = re.findall(r'merger [a-z]*', article['text'])\n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to match the preceding word as well. It would be better to match capital letters as well as lower case, which we can do with the disjunction `[a-zA-Z]`. \n",
    "\n",
    "TODO 1: complete the code below to retrieve the words that precede and follow 'merger', including capitalised and lower case words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(, article['text'])\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more useful, but we still want to retrieve whole sentences. \n",
    "\n",
    "Sentences in English are usually demarcated by punctuation, so let's use the following punctuation marks to identify sentence boundaries: '.', '!', '?'. Those punctuation marks are special characters when used in regular expressions, so to force Python to interpret them literally, we need to put the escape character '\\' in front of them. \n",
    "\n",
    "Now, we can write a disjunction that matches against the punctuation like this: `[\\.\\!\\?]`.\n",
    "\n",
    "To retrieve a whole sentence, we would like to match all of the text between two punctuation marks. Besides letters, the text of a sentence contains space characters and new line characters, which appear as '\\n'. These are formatting characters that specify a line break. To retrieve a whole sentence, we will need to match against letters, spaces and new line characters.\n",
    "\n",
    "TODO 2: Retrieve all strings containing 'merger', starting from the preceding punctuation mark, until the following punctuation mark. To do this, modify the disjunction that matches against letters to also match spaces and new line characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(, article['text'])\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have assumed the sentences consist only of letters, spaces and new lines. Can you think of any characters we have excluded here?\n",
    "\n",
    "A better way to find all matches would be to use _negation_ to match against any character _except_ the punctuation marks that bound the sentences. A negation will match any character except those specified, which we can write like this: `[^\\.\\!\\?]`, where the '^' indicates the negation.\n",
    "\n",
    "TODO 3: Modify the expression you used in the previous cell to use the negation `[^\\.\\!\\?]` to try to find all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches = []\n",
    "\n",
    "for article in train_dataset:\n",
    "    \n",
    "    ### COMPLETE THE CODE HERE\n",
    "    matches = re.findall(, article['text'])\n",
    "    ########\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        all_matches.extend(matches)\n",
    "    \n",
    "print(len(all_matches))  # length of the list of matches\n",
    "for match in set(all_matches):  # Use a set to get a list of the unique matches\n",
    "    print(match) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Did your improvement return more matching sentences? \n",
    "\n",
    "Are there any problems in our sentence segmentation? \n",
    "\n",
    "There are lots more special characters that you can use to form really powerful regular expressions. If you're interested, you can find a complete list [here](https://docs.python.org/3/library/re.html#regular-expression-syntax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Substitution\n",
    "\n",
    "We can also use regular expressions during _preprocessing_ to clean up text and prepare it for further analysis. For this, we use regular expression _substitution_, which finds a matching string within a larger piece of text, and replaces it with another string.\n",
    "\n",
    "Let's use this to clean up the text by removing the line break characters.\n",
    "\n",
    "In Python, we can use the re.sub() function, which takes three arguments:\n",
    "1. The expression to match. \n",
    "2. The pattern we should replace it with\n",
    "3. The text to apply the subtitution to. \n",
    "\n",
    "To remove the line breaks, the expression in argument 1 can be set to match any non-new line characters with the disjunction `[^\\n]*`. \n",
    "\n",
    "Here, we will also put the disjunctions inside parentheses, like this: `([^\\n]*)`. This specifies that the matching characters are a _group_. This allows the second argument (the replacement pattern) to refer to each _group_ of characters matched by the first argument. \n",
    "\n",
    "Look at the code below to see how this is done, then run it to get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL TEXT: ')\n",
    "print(train_dataset[3]['text'])\n",
    "    \n",
    "clean_article = re.sub(r'([^\\n]*)\\n([^\\n]*)', r'\\1 \\2', train_dataset[3]['text'])\n",
    "    \n",
    "print('CLEAN TEXT: ')\n",
    "print(clean_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Normalisation \n",
    "\n",
    "For most text analytics tasks, we will first need to transform the raw text to a suitable format for input to method such as a classifier. This process is called _text normalisation_ and is part of the _preprocessing_ stage. There are three common steps:\n",
    "\n",
    "1. Sentence segmentation, which we have already been doing with regular expressions. \n",
    "2. Tokenisation, in which the sentences are split into a sequence of tokens, which include words, numbers and punctuation marks.\n",
    "3. Word normalisation, in which different forms of a word are replaced by a root form.\n",
    "\n",
    "We are now going to see how to perform these steps using the NLTK library.\n",
    "\n",
    "## 3.1 Sentence Segmentation\n",
    "\n",
    "Let's start by using NLTK to split a document into sentences. This should give better results than our regular expressions above.\n",
    "\n",
    "You may get some errors from NLTK when you try to use sent_tokenize or word_tokenize further down. This is usually because you need to download and install some NLTK data. Please check the error message to find out which package is required. You probably need to install a package called 'punkt'. You can install this package by running `nltk.download('punkt')` and repeating for any other missing packages that you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "article = train_dataset[3]['text']\n",
    "\n",
    "sents = nltk.sent_tokenize(article)\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4: Use the regular expression substitution code from section 2.2 to remove the new line characters from the sentences displayed above and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sents = []\n",
    "\n",
    "for sent in sents[:5]:\n",
    "    \n",
    "    ### COMPLETE YOUR CODE HERE\n",
    "    sent = \n",
    "    #######\n",
    "    \n",
    "    print(\"<SENTENCE>\")\n",
    "    print(sent)  # print the first five sentences of this document\n",
    "    \n",
    "    clean_sents.append(sent)  # save the cleaned sentences for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenisation\n",
    "\n",
    "NLTK provides a similar function for tokenizing the text at the word level. You can find the documentation [here](https://www.nltk.org/api/nltk.tokenize.html). \n",
    "\n",
    "TODO 5: Use word_tokenize() to tokenize each of the sentences from the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = []\n",
    "\n",
    "for sent in clean_sents:\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tokens = \n",
    "    #######\n",
    "    \n",
    "    print(\"<TOKENS>\")\n",
    "    print(tokens)\n",
    "    \n",
    "    tokenized_sents.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how NLTK has handled the non-letter characters. \n",
    "* What does it do with most punctuation marks? \n",
    "* When does it not split tokens based on punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sents:\n",
    "    for tok in sent:\n",
    "        if re.search(r'[^a-zA-Z0-9]', tok):  # find the non-letter and non-digit characters\n",
    "            print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Word Normalisation\n",
    "\n",
    "Many words can appear in different forms, including: \n",
    "* Conjugated verbs\n",
    "* Plural and singular nouns\n",
    "* Common abbrevations and synonyms like \"USA\" and \"US\". \n",
    "\n",
    "Mapping all of these surface forms to a single root form reduces the size of the vocabulary that we have to deal with and can therefore improve the performance of text classifiers or topic models.\n",
    "\n",
    "The two most widely used tools for this task in English are the Porter Stemmer and WordNet Lemmatizer. These tools apply a series of regular expression substitutions to tokenised text to convert words to a standard format. \n",
    "* The Porter stemmer is much faster but just removes word prefixes and endings, which leads to some errors. It is often used when real-time or high-volume text processing is needed.\n",
    "* As well as applying regular expressions, lemmatizers look words up in a dictionary to find their root forms, so are more accurate but much slower. \n",
    "\n",
    "Let's start by applying the [Porter Stemmer class](https://www.nltk.org/_modules/nltk/stem/porter.html) to our tokenised text by calling the stem() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer() \n",
    "stemmed_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    stemmed_sent = [stemmer.stem(tok) for tok in sent]\n",
    "    \n",
    "    stemmed_sents.append(stemmed_sent)\n",
    "    \n",
    "    print(\"<STEMMED TOKENS>\")\n",
    "    print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the stemming results to lemmatisation. For this task, NLTK provides the [class WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html) with the method lemmatize(). This method takes an argument, `pos`, that determines whether the lemmatizer is applied to nouns, verbs, adjectives or adverbs.\n",
    "\n",
    "TODO 6: Use the WordNetLemmatizer to lemmatize the nouns in the tokenized sentences. Set the `pos` argument to 'n'. \n",
    "\n",
    "TODO 7: Add a second call to lemmatize() to lemmatize the verbs in the sentences as well. Set the `pos` argument to 'v'. \n",
    "\n",
    "How do the results compare with the Porter stemmer? \n",
    "\n",
    "How have the verbs in the sentences changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer() \n",
    "lemma_sents = []\n",
    "for sent in tokenized_sents:\n",
    "    \n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    lemma_sent = \n",
    "    #######\n",
    "    \n",
    "    lemma_sents.append(lemma_sent)\n",
    "    \n",
    "    print(\"<LEMMATIZED TOKENS>\")\n",
    "    print(lemma_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
