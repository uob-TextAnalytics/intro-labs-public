{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "# Introduction to Data Analytics Coursework -- Text Analytics Data Loader\n",
    "\n",
    "For this coursework, we recommend that you use your virtual environment that you created for the labs. Alternatively, create a fresh environment following the instructions in the [README.md in the intro-labs-public Github repository](https://github.com/uob-TextAnalytics/intro-labs-public). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c8361",
   "metadata": {},
   "source": [
    "# Amazon Reviews\n",
    "\n",
    "This dataset contains Amazon reviews in different languages along with their star ratings from 1 to 5. You can choose which language to work with. The code below passes the argument 'en' to load English reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8502fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (./data_cache/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9eade34c1d466c988da866df49dd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with two splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 200000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"amazon_reviews_multi\", \n",
    "    'en', # Select language of the dataset\n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with two splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8f839-6844-47d6-8997-bc076489fa9d",
   "metadata": {},
   "source": [
    "The dataset already contains a test split, which we can hold out until we have tuned our method(s), and a validation split (also called 'development' set or 'devset'), as well as the training split. \n",
    "\n",
    "The validation set can be used to compute performance of your model when tuning hyperparameters,  optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set (i.e., not to look at examples from the test set while developing the model) to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specific examples in the test set.\n",
    "\n",
    "There are several approaches to validation: instead of using the presupplised validation set, you could use [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). \n",
    "\n",
    "The text below loads each of the splits into a set of documents and a set of corresponding review score labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca009bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example training document: Arrived broken. Manufacturer defect. Two of the legs of the base were not completely formed, so there was no way to insert the casters. I unpackaged the entire chair and hardware before noticing this. So, I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review of part of a chair I never got to sit in. I will go so far as to include a picture of what their injection molding and quality assurance process missed though. I will be hesitant to buy again. It makes me wonder if there aren't missing structures and supports that don't impede the assembly process.\n",
      "Corresponding review score: 1\n",
      "Number of training instances: 200000\n",
      "Number of validation instances: 5000\n",
      "Number of test instances: 5000\n"
     ]
    }
   ],
   "source": [
    "train_documents = dataset[\"train\"]['review_body']\n",
    "print(f'Example training document: {train_documents[0]}')\n",
    "train_labels = dataset[\"train\"]['stars']\n",
    "print(f'Corresponding review score: {train_labels[0]}')\n",
    "print(f'Number of training instances: {len(train_documents)}')\n",
    "\n",
    "val_documents = dataset[\"validation\"]['review_body']\n",
    "print(f'Number of validation instances: {len(val_documents)}')\n",
    "val_labels = dataset[\"validation\"]['stars']\n",
    "\n",
    "test_documents = dataset[\"test\"]['review_body']\n",
    "print(f'Number of test instances: {len(test_documents)}')\n",
    "test_labels = dataset[\"test\"]['stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af3f05-fea5-4adc-ac0a-7e913998abbe",
   "metadata": {},
   "source": [
    "The star ratings are our classes in this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d07354e-64b7-441b-90f1-4b18cea75b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd37970",
   "metadata": {},
   "source": [
    "The training set is very large, so you may wish to work with a subset of the training data by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f6f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split test data from training data\n",
    "train_documents, unused_documents, train_labels, unused_labels = train_test_split(\n",
    "    train_documents, \n",
    "    train_labels, \n",
    "    test_size=0.9, \n",
    "    stratify=train_labels  # make sure the same proportion of labels is in the test set and training set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3af369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances in the train dataset? \n",
      "\n",
      "20000\n",
      "\n",
      "What does one instance look like? \n",
      "\n",
      "Other than the missing oil and a few missing attachments, it's great.\n"
     ]
    }
   ],
   "source": [
    "# label 0 = negative, 1 = neutral, 2 = positive\n",
    "print(f'How many instances in the train dataset? \\n\\n{len(train_documents)}')\n",
    "print('')\n",
    "print(f'What does one instance look like? \\n\\n{train_documents[234]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af0fbc-2c18-4b68-839d-802595dac600",
   "metadata": {},
   "source": [
    "# Bio Creative V\n",
    "\n",
    "This dataset contains sentences extracted from scientific articles on PubMed. The sentences are annotated with two types of entities, chemicals and diseases. Let's load the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d0376c-93d1-4ee9-a7ba-76bba2afc2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bc5_cdr (./data_cache/tner___bc5_cdr/bc5cdr/1.0.0/66ea1a8c1cb0adcf8751ab7993f5d31717f21c2a9b89e21506fe959d904a7bf6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56793185f3540ae8f269184bfa94eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with 3 splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5228\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5330\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'tags'],\n",
      "        num_rows: 5865\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"tner/bc5cdr\", \n",
    "    cache_dir='./data_cache'\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with {len(dataset)} splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e775ce-e4e1-47df-818b-a029c9a56156",
   "metadata": {},
   "source": [
    "The data is also split into train, validation and test. It may be convenient to reformat these splits into lists of tokens and lists of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8515ff00-f8bc-42ac-801f-50224ea50e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_ner = [item['tokens'] for item in dataset['train']]\n",
    "train_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['train']]\n",
    "\n",
    "val_sentences_ner = [item['tokens'] for item in dataset['validation']]\n",
    "val_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['validation']]\n",
    "\n",
    "test_sentences_ner = [item['tokens'] for item in dataset['test']]\n",
    "test_labels_ner = [[str(tag) for tag in item['tags']] for item in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01ba63f-d17e-457b-94b1-999061d1d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 5228\n",
      "Number of validation instances: 5330\n",
      "Number of test instances: 5865\n",
      "Example training sentence (already tokenised): ['Naloxone', 'reverses', 'the', 'antihypertensive', 'effect', 'of', 'clonidine', '.']\n",
      "...corresponding tags for the same example: ['1', '0', '0', '0', '0', '0', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training instances: {len(train_sentences_ner)}')\n",
    "print(f'Number of validation instances: {len(val_sentences_ner)}')\n",
    "print(f'Number of test instances: {len(test_sentences_ner)}')\n",
    "\n",
    "print(f'Example training sentence (already tokenised): {train_sentences_ner[0]}')\n",
    "print(f'...corresponding tags for the same example: {train_labels_ner[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529ab09-9747-4d48-aa65-c17aec2dae79",
   "metadata": {},
   "source": [
    "These are the tags used to annotate the entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7f29a3-3c27-4919-8531-59b6bee7b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Chemical', 2: 'B-Disease', 3: 'I-Disease', 4: 'I-Chemical'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    \"O\": 0,\n",
    "    \"B-Chemical\": 1,\n",
    "    \"B-Disease\": 2,\n",
    "    \"I-Disease\": 3,\n",
    "    \"I-Chemical\": 4\n",
    "}\n",
    "\n",
    "label2id = {v:k for k, v in id2label.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39631465-8c97-4b2c-8e62-bcb6f7beebb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
