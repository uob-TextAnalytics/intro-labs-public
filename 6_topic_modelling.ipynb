{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Topic Modelling\n",
    "\n",
    "This lab explores topic modelling with Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "* Be able to apply Latent Dirichlet Allocation (LDA) using the Gensim library\n",
    "* Know how to interpret the outputs of LDA for a specific document or topic \n",
    "* Know how to apply TF-IDF to improve the vector representations of the documents.\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Implementing LDA Topic Modelling\n",
    "* TF-IDF vectors\n",
    "* Visualizing Topic Modelling Results\n",
    "* Optional: HDP model as an alternative to LDA\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. Look out for 'QUESTIONS' which you should try to answer before moving on to the next cell. Aim to work through the lab during the scheduled lab hours. To get help, you can talk to TAs or the lecturer during the labs, post questions to Blackboard (anonymously) or on Teams in the QA channel (with your name), or ask a question in the Wednesday live sessions. \n",
    "\n",
    "As you work through the notebooks, please make a note of any code that is unclear to you.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises! To understand what's going on inside the methods we use here, make sure to watch the lecture videos for the same week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling is uses unsupervised learning to extract the topics (represented as a probability distribution over words) that occur in a collection of documents. Let's load some data to apply topic modelling to from the 20 newsgroups dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We will use the 'train' split for learning the topics in an unsupervised manner:\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# We will apply our learned topic model to the 'test' split later on:\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newsgroups are internet discussion groups, where users discuss a range of topics. Despite the name, the posts do not usually contain news. This dataset contains posts from 20 different newsgroups. Each newsgroup has a particular theme. We can view the list of newsgroups in the training split as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a new dataset, it is always good to play around with the data and see what we have, and how to find everything :) \n",
    "```newsgroups_train``` is a Python dictionary. Let's look at the keys and the types of objects stored in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in newsgroups_train:\n",
    "    print(i, type(newsgroups_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```data``` item is a list of raw text documents. This is all we need for topic modelling, so we can ignore the ```target``` and ```DESCR``` keys for now.\n",
    "\n",
    "The code below prints out the first post in ```data```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DOCUMENT 0:')\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply topic modelling, we need to first preprocess the data. We will carry out the following steps using the same approach as previous labs:\n",
    "* Tokenise the posts using NLTK's word_tokenize() function\n",
    "* Remove non-word tokens and tokens with length less than 3 (likely to be numbers and punctuation that are not related to specific topics) and longer than 15 (probably URLs, codes, andbadly formatted tokens rather than proper words)\n",
    "* Convert the tokens to lower case\n",
    "* Remove stopwords: we have not used this step before; it removes tokens such as 'the' and 'a' that appear in a list of very common words, because these words do not tell us much about topics\n",
    "* Lemmatize the tokens using WordNetLemmatizer to convert verbs to their root forms\n",
    "\n",
    "We're going to introduce another library, [Gensim](https://radimrehurek.com/gensim/), which contains a lot of useful tools for topic modelling, text normalisation, and vector representations of words or documents (embeddings). For preprocessing, we will use the list of stopwords provided by Gensim. For topic modelling, removing stopwords can be particularly useful to remove noise and reduce the model complexity. Run the code below to preprocess the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS # find stopwords\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(400)  # We fix the random seed to ensure we get consistent results when we repeat the lab.\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in simple_preprocess(text) :  # Tokenize, remove very short and very long words, convert to lower case, remove words containing non-letter characters\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token, 'v'))\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Create a list of preprocessed documents\n",
    "processed = []\n",
    "for doc in newsgroups_train.data:\n",
    "    processed.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished the preprocessing, we need to construct the input for Gensim's topic modelling method. We do so by constructing a dictionary with word<->id mappings, then converting that into a bag of words, which will be the input to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(processed) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(dictionary)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Now we are ready to perform topic modelling using LDA. \n",
    "\n",
    "We are going to try 20 topics in the document corpus: the number of newsgroups is also 20, so perhaps we will find topics that correspond with the newsgroups. We will be running LDA using all CPU cores to parallelize and speed up model training.\n",
    "\n",
    "Gensim provides the ```LdaModel``` class. When we construct an ```LdaModel``` object, some of the parameters we will be tweaking are:\n",
    "   * *num_topics*, the number of requested latent topics to be extracted from the training corpus. <br>\n",
    "   * *id2word*, a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing. <br>\n",
    "   * *workers*, the number of extra processes to use for parallelization. Uses all available cores by default. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_model =  LdaModel(bow_corpus, \n",
    "                      num_topics=20, \n",
    "                      id2word=dictionary,                                    \n",
    "                      passes=10,\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.1: run the code below to print out the topic distributions found by LDA. Can you find any topics that relate to specific newsgroups (newsgroups_train.target_names)? Can you find any other meaningful topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.2: What do the values beside each word mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained LDA mode, we can run it on a new, unseen document to get the breakdown of topics. Let's test the model on a document from the test set. First, get the raw document, then apply preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document_idx = 0\n",
    "unseen_document = newsgroups_test.data[test_document_idx]\n",
    "print(unseen_document)\n",
    "\n",
    "print(f' This document is from newsgroup {newsgroups_test.target_names[newsgroups_test.target[test_document_idx]]}')\n",
    "\n",
    "# Data preprocessing step for the unseen document - It is the same preprocessing we have performed for the training data\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for idx, count in bow_vector:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run our preprocessed document through the LDA model as follows to obtain $\\boldsymbol{\\theta}^d$, the topic distribution (topics with zero probability are not shown):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the word-topic distributions for the topics associated with this document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = lda_model[bow_vector]\n",
    "\n",
    "for index, probability in sorted(topic_distribution, key=lambda tup: -1*tup[1]):\n",
    "    print(\"Index: {}\\nProbability: {}\\t Topic: {}\".format(index, probability, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Topics \n",
    "\n",
    "Let's compare the topics for some training set documents in the same newsgroup.\n",
    "\n",
    "TODO 3.3: Complete the code below to define a function to retrieve the topic distributions for 10 documents in the ```talk.politics.mideast``` newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import any2sparse\n",
    "\n",
    "def get_document_ids_in_newsgroup(newsgroup_name, newsgroups_data):\n",
    "    # retrieve a list of document indexes for documents with this target_name\n",
    "    doc_idxs = []\n",
    "    for i, target_i in enumerate(newsgroups_data.target):\n",
    "        if newsgroups_data.target_names[target_i] == newsgroup_name:\n",
    "            doc_idxs.append(i)\n",
    "            \n",
    "    #print(\"There are {} documents in the newsgroup {}\".format(len(doc_idxs), newsgroup_name))\n",
    "            \n",
    "    return doc_idxs\n",
    "\n",
    "\n",
    "def get_topic_dists_in_newsgroup(newsgroup_name, lda_model, max_num_docs=10):\n",
    "    doc_idxs = get_document_ids_in_newsgroup(newsgroup_name, newsgroups_train)\n",
    "    \n",
    "    # only use the first ten documents.\n",
    "    if len(doc_idxs) > max_num_docs:\n",
    "        doc_idxs = doc_idxs[:max_num_docs]\n",
    "    print(doc_idxs)\n",
    "    \n",
    "    # Save each theta_d distribution to the list 'thetas':\n",
    "    thetas = []\n",
    "    \n",
    "    for doc_idx in doc_idxs:\n",
    "        ### COMPLETE THE CODE HERE\n",
    "        \n",
    "        # Get the document from newsgroups_train\n",
    "        \n",
    "        # Apply preprocessing to get a bag-of-words vector:\n",
    "        \n",
    "        # Use the LDA model to compute the topic distribution for this document:\n",
    "        \n",
    "        #######################\n",
    "        \n",
    "        thetas.append(theta_d)\n",
    "    \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function above and print out the topic distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = get_topic_dists_in_newsgroup('talk.politics.mideast', lda_model, max_num_docs=10)\n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text output is quite hard to read. We can improve on it by making a bar chart with a distinct colour for each topic. The function below can be used to plot a simple bar chart using the ```matplotlib``` library. This will help us see which documents discuss the same topics. The height of each bar is the probability for that topic within the document, according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# choose some colours for the topics\n",
    "colours = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'teal', 'pink', 'purple',\n",
    "           'orange', 'gray', 'lime', 'darkgreen', 'lightgray', 'navy', 'gold', 'crimson', 'darkgray', 'fuchsia']\n",
    "\n",
    "def convert_theta_sparse_to_dense(theta_d_sparse, num_topics):\n",
    "    theta_d = np.zeros(num_topics)  # an empty array\n",
    "    \n",
    "    # split the output from lda_model into two lists\n",
    "    active_topics_in_d, probs = map(list, zip(*theta_d_sparse))\n",
    "    \n",
    "    # record the values in theta_d\n",
    "    for i, topic in enumerate(active_topics_in_d):\n",
    "        if topic >= num_topics:\n",
    "            break\n",
    "            \n",
    "        theta_d[topic] = probs[i]\n",
    "    \n",
    "    return theta_d\n",
    "\n",
    "# a function for producing a bar chart for a document\n",
    "def plot_theta(thetas, d, num_docs, num_topics):\n",
    "    plt.subplot(int(num_docs/3) + 1, 3, d+1)   # make a set of subplots inside a figure, with four subplots per row\n",
    "    \n",
    "    theta_d = convert_theta_sparse_to_dense(thetas[d], num_topics)\n",
    "    \n",
    "    # plot the results so that the same topics always occur at the same place along the x axis.\n",
    "    plt.bar(x=np.arange(len(theta_d)), height=theta_d, color=colours, tick_label=np.arange(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.4: use the function to plot the topic distributions of the 10 documents we selected from the ```talk.politics.mideast``` newsgroup. What do you notice? Are there any topics the documents have in common? Any they do not? Refer back to the printed list of topics above to find the most common topic in this newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "############################\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at how the topics we've found with LDA relate to the newsgroups (the targets in the training set).\n",
    "\n",
    "The code below iterates over the newsgroups, computing the topic distribution for each document in that particular newsgroup. \n",
    "\n",
    "TODO 3.5: Complete the function below to compute the mean topic distribution, $\\boldsymbol\\theta$, for each newsgroup. Hint: use the ```mean``` function from Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array which will store a matrix of values. Rows correspond to newsgroups and columns to LDA topics.\n",
    "def get_newsgroups_mean_topics(lda_model, num_topics):\n",
    "    \n",
    "    # Create a matrix where each row corresponds to a newsgroup, and each column to a topic.\n",
    "    # In each entry, we will save the mean probability of the topic for the documents in that newsgroup.\n",
    "    mean_thetas = np.zeros((len(newsgroups_train.target_names), num_topics))\n",
    "\n",
    "    print(mean_thetas.shape)\n",
    "    for t, target_name in enumerate(newsgroups_train.target_names):\n",
    "        # Obtain the thetas for the documents with this target name\n",
    "        thetas_t_sparse = get_topic_dists_in_newsgroup(target_name, lda_model, max_num_docs=10)\n",
    "        print(np.array(thetas_t_sparse))\n",
    "        \n",
    "        # convert the thetas to a dense vector format\n",
    "        thetas_t = []\n",
    "        for theta_d_t_sparse in thetas_t_sparse:\n",
    "            if not theta_d_t_sparse:\n",
    "                continue  # if it's empty\n",
    "            thetas_d = convert_theta_sparse_to_dense(theta_d_t_sparse, num_topics)\n",
    "            thetas_t.append(thetas_d)\n",
    "            \n",
    "        # compute the mean theta for this newsgroup and store it in mean_thetas \n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "        \n",
    "        ###########################\n",
    "\n",
    "        #print(mean_thetas[t])\n",
    "    return mean_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a matrix ```mean_thetas``` using ```matplotlib``` using the code below.\n",
    "\n",
    "TODO 3.6: which LDA topics are common across many newsgroups? Are there any topics that are specific to particular newsgroups? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_newsgroup_topic_matrix(model, num_topics):\n",
    "    # Run the function we defined above to get the mean topic distributions:\n",
    "    mean_thetas = get_newsgroups_mean_topics(model, num_topics)\n",
    "    \n",
    "    print(f'mean_thetas is a matrix of shape {mean_thetas.shape}')\n",
    "\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(20,10))\n",
    "    clear_output()\n",
    "\n",
    "    # Plot the matrix as a 2-D grid, where colours represent the values.\n",
    "    plt.imshow(mean_thetas)\n",
    "\n",
    "    # Change the labels on the axes\n",
    "    plt.yticks(range(len(newsgroups_train.target_names)), newsgroups_train.target_names )\n",
    "    plt.xticks(range(num_topics))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_newsgroup_topic_matrix(lda_model, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Cosine Similarity and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many tasks it is useful to compute similarity between documents, for example, if we want cluster documents into groups or to retrieve some documents that are similar to the one we are currently reading. In order to compute similarity or distance, we need to represent documents as numerical vectors. The code we have run so far produces bag-of-words vectors for each document, where each entry in vector is the count of word in the vocabulary.\n",
    "\n",
    "Let's take a document from the 'rec.autos' newsgroup as a 'query' document and compare it to two others using cosine similarity. We'll take another one from 'rec.autos' and one from 'misc.forsale'. First let's show the documents and their bag of words vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our first document\n",
    "doc_idxs = get_document_ids_in_newsgroup('rec.autos', newsgroups_train)\n",
    "doc_idx_0 = doc_idxs[19]\n",
    "print(newsgroups_train.data[doc_idx_0].strip())\n",
    "query_doc = bow_corpus[doc_idx_0]\n",
    "# show the bag of words vector in sparse format. In each pair of numbers, the first is the word ID and the second is the word count.\n",
    "for idx, count in query_doc:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a comparison document from the same newsgroup\n",
    "doc_idx_1 = doc_idxs[17]\n",
    "comparison_doc_1 = bow_corpus[doc_idx_1]\n",
    "print(newsgroups_train.data[doc_idx_1].strip())\n",
    "# show the bag of words vector in sparse format. In each pair of numbers, the first is the word ID and the second is the word count.\n",
    "for idx, count in comparison_doc_1:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get another comparison document from another newsgroup\n",
    "doc_idxs = get_document_ids_in_newsgroup('misc.forsale', newsgroups_train)\n",
    "doc_idx_2 = doc_idxs[2]\n",
    "comparison_doc_2 = bow_corpus[doc_idx_2]\n",
    "print(newsgroups_train.data[doc_idx_2].strip())\n",
    "\n",
    "# show the bag of words vector in sparse format. In each pair of numbers, the first is the word ID and the second is the word count.\n",
    "for idx, count in comparison_doc_2:\n",
    "    print(f'{dictionary[idx]}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the similarity between the documents, we use cosine similarity:\n",
    "\n",
    "$$similarity<v_1, v_2> = \\frac{v_1 \\cdot v_2}{|| v_1 || \\cdot || v_2 ||}$$\n",
    "\n",
    "\n",
    "TODO 4.1: Which document do you expect to have higher similarity to the query? Run the code below and see if it meets your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import cossim\n",
    "\n",
    "cos_sim1 = cossim(query_doc, comparison_doc_1)\n",
    "print(f'The cosine similarity between documents {doc_idx_0} and {doc_idx_1} is: {cos_sim1}')\n",
    "\n",
    "cos_sim2 = cossim(query_doc, comparison_doc_2)\n",
    "print(f'The cosine similarity between documents {doc_idx_0} and {doc_idx_2} is: {cos_sim2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last documents share some common words, despite discussing different topics. We can alter our vector representations to focus more on keywords by using TF-IDF instead of bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is computed from two terms. First, the log of the term frequency:\n",
    "$$ tf(t,d) = count(t,d)$$\n",
    "\n",
    "We multiple the log term frequency by the inverse document frequencey, which is computed like this:\n",
    "\n",
    "$$ idf(t) = \\log_{2}\\frac{N}{df(t)}$$\n",
    "\n",
    "Gensim provides the TfidfModel class to compute TF-IDF vectors from our existing corpus object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.2: Apply the tfidf_model to `query_doc`, `comparison_doc_1` and `comparison_doc_2` to obtain a TF-IDF vector for each document. Print the results. Hint: you can apply the model in the same was a you applied lda_model to a bow_vector for an unseen test document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4.3: Compute the cosine similarity between the query document and the two comparison documents using the TF-IDF vectors. What do you notice about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optional: Hierarchical Dirichlet Process (HDP)\n",
    "\n",
    "This section is optional if you want to learn more about HDP as an alternative to LDA. \n",
    "\n",
    "There is an implementation of the [HDP model provided by gensim](https://radimrehurek.com/gensim/models/hdpmodel.html). Instead of passing in a fixed number of topics, HDP will try to learn a good number of topics to fit the data.\n",
    "\n",
    "OPTIONAL TODO 5.1: Refer to the documentation for HDP and train an HDP model. Hint: reuse the ```bow_corpus``` and ```dictionary``` as arguments in the same way that you did to construct the ```LdaModel``` object.\n",
    "\n",
    "Use the trained HDP model to obtain mean topic distributions for each newsgroup in the test set with ```get_newsgroups_mean_topics()```. Plot the mean topic matrix as above and compare it to the results from LDA. Set alpha and gamma to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# print the word-topic distributions for \n",
    "for idx, topic in hdp_model.print_topics(20):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell shows the first 20 topics. HDP learns the number of topics that are needed to model the dataset. It produces a global distribution over the topics. Topics with very low probability can be considered inactive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_global_topic_weights(hdp_model):\n",
    "    global_topic_weights = hdp_model.m_varphi_ss / np.sum(hdp_model.m_varphi_ss)\n",
    "\n",
    "    plt.bar(np.arange(len(global_topic_weights)), global_topic_weights)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Topic ID')\n",
    "    \n",
    "plot_global_topic_weights(hdp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to visualise the topics that HDP finds for each newsgroup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_newsgroup_topic_matrix(hdp_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha and gamma arguments to HdpModel control the 'concentration' of topics. Varying these parameters therefore affects the number of topics that HDP finds -- whether it tends towards many fine-grained topics, or few coarse-grained topics.\n",
    "\n",
    "OPTIONAL TODO 5.2: Change the values of alpha and gamma for the HDP model and see what their effect is. Hint: to see noticable differences, change the values by a factor of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "\n",
    "###\n",
    "\n",
    "# print the word-topic distributions for \n",
    "for idx, topic in hdp_model.print_topics(20):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "plot_global_topic_weights(hdp_model2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_newsgroup_topic_matrix(hdp_model2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
