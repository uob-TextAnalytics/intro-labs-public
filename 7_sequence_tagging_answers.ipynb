{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 7: Sequence Labelling\n",
    "\n",
    "This will be our last lab of the unit -- after this we focus on the coursework. You may wish to split this lab over the next two weeks.\n",
    "\n",
    "In this lab we will implement an HMM for part-of-speech (POS) tagging, then see how to use a library to run an HMM and a CRF for named entity recognition.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to implement the main parts of a supervised HMM.\n",
    "* Understand what the steps of Viterbi are doing.\n",
    "* Know how to use a CRF and specify the features it uses.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part uses CRFs for the task of named entity recognition (to be covered in week 8).\n",
    "\n",
    "## Background\n",
    "\n",
    "### Hidden Markov Model (HMM)\n",
    "\n",
    "In this lab we will look into Hidden Markov Model (HMM) to model sequential data. HMMs are based on the Markov assumption which states that the present state $z_n$ is sufficient to predict the future $y_{n+1}$ so the past $y_{0:n-1}$ can be forgotten.\n",
    "\n",
    "Often, the states we are interested in cannot be observed directly -- they are 'hidden'. As we will see in Exercise 2, the part-of-speech (POS) tags are hidden states that we want to predict. We can only observe the words, and have to use them to infer the tags. \n",
    "An HMM is specified by the following components:\n",
    "\n",
    "* A set of $N$ states.\n",
    "\n",
    "* A transition probability matrix $A$ where each element $a_{ij}$ represents the probability of moving from state $i$ to state $j$, s.t.  $ \\sum^{N}_{j=1} a_{ij} = 1$ $ \\forall i$\n",
    "\n",
    "* An emission probability distribution, the probabilities of observations $x_n$ being generated from a state $y_n$\n",
    "\n",
    "* An initial probability distribution over states. $\\pi_n$ is the probability that the Markov chain will start in state $n$. Also, $\\sum^{N}_{n=1} \\pi_n = 1 $\n",
    "\n",
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging enables the extraction of meaningful information about words in a sentence and their relation to neighbouring words. Parts of speech are useful features for labeling named entities like people or organisations in information extraction. A word’s part of speech can even play a role in speech recognition or speech synthesis, e.g., the word 'content' is pronounced CONtent when it is a noun and conTENT when it is an adjective.\n",
    "\n",
    "POS tagging is the process of assigning a POS tag to each token in a text. The input to a tagging algorithm is a sequence of tokenised words and the output is a sequence of tags, one per token. Many words can have different POS tags in difference contexts, so the goal is to find the correct tag for a particular situation. For example, \"book\" can be a verb (\"book that flight\") or a noun (\"hand me that book\"). POS-tagging resolves these ambiguities by choosing the proper tag for the context. POS-tagging is a specific case of the more generic NLP task of sequence labelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "For POS tagging, we are going to start with the [Brown corpus](https://www.nltk.org/nltk_data/), which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967.\n",
    "\n",
    "If you would like to try out POS tagging in another language, just uncomment the code below to switch to the [NLTK Indian corpus](https://www.nltk.org/_modules/nltk/corpus/reader/indian.html), which contains datasets for POS tagging in Bangla, Hindi, Marathi and Telugu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/es1595/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# If you want to try another language, try Hindi:\n",
    "# nltk.download('indian')  # the dataset\n",
    "# from nltk.corpus import indian\n",
    "# nltk_data = list(indian.tagged_sents('hindi.pos'))\n",
    "\n",
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')   # download the POS tags data\n",
    "from nltk.corpus import brown\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the dataset into training and test sets. Run the following cells to achieve that and to prepare the dataset in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 45872\n",
      "Number of test sentences: 11468\n",
      "Number of training sentences in train_toks: 45872\n",
      "Number of test sentences in test_toks: 11468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,  # use 80% as the training data\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []  # each item in the list is a list of tokens in a document\n",
    "train_tags = []  # each item in the list is a list of corresponding tags\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of training sentences in train_toks: {len(train_toks)}')\n",
    "print(f'Number of test sentences in test_toks: {len(test_toks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many words the vocabulary has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tagged tokens in the training set: 927092\n",
      "Number of tagged tokens in the test set: 234100\n"
     ]
    }
   ],
   "source": [
    "# create list of train and test tagged words\n",
    "print(f'Number of tagged tokens in the training set: {len([ tok for sent in train_toks for tok in sent ])}')\n",
    "print(f'Number of tagged tokens in the test set: {len([ tok for sent in test_toks for tok in sent ])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's expore the different types of tags by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible tags: 12\n",
      "Possible tags: {'PRT', '.', 'ADP', 'PRON', 'DET', 'NUM', 'VERB', 'X', 'ADJ', 'CONJ', 'ADV', 'NOUN'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = {tag for sent in train_tags for tag in sent}\n",
    "print(f'Number of possible tags: {len(unique_tags)}')\n",
    "print(f'Possible tags: {unique_tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 1.1: Find out what the tags mean at https://github.com/slavpetrov/universal-pos-tags.\n",
    "\n",
    "The next cell shows an exampes sentence from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence example: [('many', 'ADJ'), ('of', 'ADP'), ('their', 'DET'), ('gifted', 'ADJ'), ('members', 'NOUN'), ('were', 'VERB'), ('prominent', 'ADJ'), ('in', 'ADP'), ('the', 'DET'), ('Vatican', 'NOUN'), ('as', 'ADP'), ('physicians', 'NOUN'), (',', '.'), ('musicians', 'NOUN'), (',', '.'), ('bankers', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence example: {}'.format(train_set[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a vocabulary and convert each token to its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: [41, 28, 46, 40, 42, 47, 45, 23, 31, 37, 38, 44, 11, 43, 11, 39, 0]\n",
      "Size of vocabulary is 56057\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Convert the tokens to IDs in a vocabulary ready for input to our models\n",
    "dictionary = Dictionary(train_toks + test_toks)\n",
    "\n",
    "train_toks_encoded = [dictionary.doc2idx(sent) for sent in train_toks]\n",
    "test_toks_encoded = [dictionary.doc2idx(sent) for sent in test_toks]\n",
    "print(f'Example sentence: {train_toks_encoded[3]}')\n",
    "\n",
    "V = len(dictionary.values())  # vocabulary\n",
    "print(f'Size of vocabulary is {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert the tags to their indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert the tags from their names to numbers\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in train_tags for tag in sentence])\n",
    "train_tags_encoded = [tag_encoder.transform(sentence) for sentence in train_tags]\n",
    "test_tags_encoded = [tag_encoder.transform(sentence) for sentence in test_tags]\n",
    "\n",
    "num_tags = len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON',\n",
       "       'PRT', 'VERB', 'X'], dtype='<U4')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Implementing the HMM\n",
    "\n",
    "The two main components of HMMs are the transition model and the observation (or emission) model. The transition model estimates $P(tag_{t+1}|tag_t)$, the probability of the next tag given the current tag. For discrete features, such as tokens, the observation model is the same as the naïve Bayes classifier that we covered in lab 2. It estimates $P(word|tag_t)$, the probability of observing a word given the current tag. \n",
    "\n",
    "Let's start by implementing the transition matrix. \n",
    "\n",
    "TODO 2.1: Count the state (tag) transitions and starting state (tag) occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # progress bars\n",
    "\n",
    "transitions = np.zeros((num_tags, num_tags))\n",
    "start_states = np.zeros(num_tags)\n",
    "\n",
    "for sentence_tags in tqdm(train_tags_encoded):\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i==0:\n",
    "            start_states[tag] += 1\n",
    "            continue\n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        transitions[sentence_tags[i-1], tag] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO 2.2: Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix $A$ and starting state probabilities $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "transitions /= np.sum(transitions, 1)[:, None]\n",
    "start_states /= np.sum(start_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's compute the emission/observation model.\n",
    "\n",
    "TODO 2.3: Count the number of occurrences of each word type given each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45872it [00:00, 69293.68it/s]\n"
     ]
    }
   ],
   "source": [
    "observations = np.zeros((num_tags, V))\n",
    "\n",
    "for i, sentence_toks in tqdm(enumerate(train_toks_encoded)):\n",
    "    sentence_tags = train_tags_encoded[i]\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        # WRITE YOUR OWN CODE HERE\n",
    "        observations[tag, tok] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO 2.4: Normalise the observation counts to obtain the observation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#WRITE YOUR OWN CODE HERE\n",
    "observations /= np.sum(observations, 1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To predict the most likely sequence of tags, we use the Viterbi algorithm, defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = len(observed_seq)\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO 2.6: Use the viterbi function to estimate the most likely sequence of states on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11468/11468 [00:24<00:00, 463.54it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for sentence in tqdm(test_toks_encoded):\n",
    "    # WRITE YOUR OWN CODE HERE\n",
    "    predictions.append(viterbi(sentence, num_tags, start_states, transitions, observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11468/11468 [00:00<00:00, 19391.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the sequence of tag IDs to tag names\n",
    "predicted_tags = []\n",
    "for sequence in tqdm(predictions):\n",
    "    predicted_tags.append(tag_encoder.inverse_transform(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 2.7: Run the code below to print some example predictions. What kinds of errors does the method make? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:      ['``', 'My', 'God', ',', \"I'm\", 'shot', \"''\", '!', '!']\n",
      "Gold tag:    ['.', 'DET', 'NOUN', '.', 'PRT', 'VERB', '.', '.', '.']\n",
      "Predictions: ['.' 'DET' 'NOUN' '.' 'PRT' 'NOUN' '.' '.' '.']\n",
      "Tokens:      ['She', 'thought', 'she', 'was', 'bigger', 'than', 'we', 'are', 'because', 'she', 'came', 'from', 'Torino', \"''\", '.']\n",
      "Gold tag:    ['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', 'NOUN', '.', '.']\n",
      "Predictions: ['PRON' 'VERB' 'PRON' 'VERB' 'ADJ' 'ADP' 'PRON' 'VERB' 'ADP' 'PRON' 'VERB'\n",
      " 'ADP' 'NOUN' '.' '.']\n",
      "Tokens:      ['Meanwhile', ',', 'I', 'reloaded', 'my', 'gun', ',', 'as', 'the', 'other', 'men', 'were', 'doing', '.']\n",
      "Gold tag:    ['ADV', '.', 'PRON', 'VERB', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.']\n",
      "Predictions: ['ADV' '.' 'PRON' 'VERB' 'DET' 'NOUN' '.' 'ADP' 'DET' 'ADJ' 'NOUN' 'VERB'\n",
      " 'VERB' '.']\n",
      "Tokens:      ['The', 'difficulty', 'was', 'that', 'each', 'day', 'seemed', 'to', 'produce', 'its', 'quota', 'of', 'details', 'which', 'must', 'be', 'cleaned', 'up', 'immediately', '.']\n",
      "Gold tag:    ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'DET', 'VERB', 'VERB', 'VERB', 'PRT', 'ADV', '.']\n",
      "Predictions: ['DET' 'NOUN' 'VERB' 'ADP' 'DET' 'NOUN' 'VERB' 'PRT' 'VERB' 'DET' 'NOUN'\n",
      " 'ADP' 'NOUN' 'DET' 'VERB' 'VERB' 'VERB' 'PRT' 'ADV' '.']\n"
     ]
    }
   ],
   "source": [
    "# print some examples:\n",
    "examples = [2, 334, 4983, 2389]\n",
    "for eg in examples:\n",
    "    print(f'Tokens:      {test_toks[eg]}')\n",
    "    print(f'Gold tag:    {test_tags[eg]}')\n",
    "    print(f'Predictions: {predicted_tags[eg]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9436992738146092\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition (NER) with CRF\n",
    "\n",
    "Named entity recognition is the task of identifying entities from text, such as people, locations, organisations and times. It is usually modelled as a sequence labelling task. However, many entities span more than one token. To show where these spans start and end, we therefore tag each token as either 'outside' (not part of an entity), 'beginning' or 'inside' (continuation of an entity span). Beginning and inside tags also have the entity type, e.g., \"B-Person\" or \"I-Location\". \n",
    "\n",
    "We will learn more about NER next week, so you can continue the lab next week if you prefer. Here we will use an NER dataset to learn about CRFs.\n",
    "\n",
    "Let's load some NER data consisting of English news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (./data_cache/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 14042 instances loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"train\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (./data_cache/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 3454 instances loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"test\",\n",
    "    #ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the instances in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as NER tags, the dataset includes POS tags and Chunk tags, which identify the grammatical phrases in the sentence.\n",
    "\n",
    "The tags are all stored by their indexes. The mapping of the POS tags is:\n",
    "\n",
    "```\n",
    "{'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    " ```\n",
    " \n",
    "The mapping from indexes to NER tags is:\n",
    "\n",
    "```\n",
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag_mapping = {0: 'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the NER data in the right format for NLTK's CRFTagger class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in train_dataset][:-1]\n",
    "test_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in test_dataset][:-1]\n",
    "test_tokens = [s['tokens'] for s in test_dataset][:-1]\n",
    "test_tags = [[ner_tag_mapping[tok] for tok in s['ner_tags']] for s in test_dataset][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a CRF tagger on our training set. The method you need to use from NLTK is the [train method of the conditional random field (CRF)](https://www.nltk.org/_modules/nltk/tag/crf.html). You need to call the constructor with default arguments, then the train() function.\n",
    "\n",
    "TODO 3.1: Write a function to train and return a CRF named entity recogniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = nltk.tag.CRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some predictions from the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the tagger is performing. In NER, we evaluate performance by finding correctly matched entities, rather than correctly tagged tokens. Only an exact entity match counts as correct. Therefore, we need to compute precision, recall and F1 score by computing true positives, false positives and false negatives by looking for the predicted entity spans and the gold-labelled entity spans in the test set.\n",
    "\n",
    "The code below contains a function that extract a list of spans from the tagged sentences. The next function calls extract_spans() and computes the precision, recall and f1 scores. However, the function is incomplete.\n",
    "\n",
    "Run the cal_span_level_F1() function below to compute span-level F1 scores for the predictions. Have a look at the results. Which types of entity are being recognised well and which are very poor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.7971698113207547\n",
      "F1 score for class PER = 0.7682375726275017\n",
      "F1 score for class MISC = 0.6945925361766947\n",
      "F1 score for class ORG = 0.6524098308330674\n",
      "Macro-average f1 score = 0.7281024377395046\n"
     ]
    }
   ],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans\n",
    "\n",
    "\n",
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to help the CRF tagger by adding some more features. Part-of-speech tags often provide useful information for identifying entites. The code below defines a modified CRFTagger class that overwrites the ```_get_features()``` method, which extracts the features from the tokens. \n",
    "\n",
    "TODO 3.2: Add in the previous and next works as features. Be careful with the start and end of the sequence where there is no previous or next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "class CustomCRFTagger(nltk.tag.CRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, idx):\n",
    "            \"\"\"\n",
    "            Extract basic features about this word including\n",
    "                - Current word\n",
    "                - is it capitalized?\n",
    "                - Does it have punctuation?\n",
    "                - Does it have a number?\n",
    "                - Suffixes up to length 3\n",
    "\n",
    "            Note that : we might include feature over previous word, next word etc.\n",
    "\n",
    "            :return: a list which contains the features\n",
    "            :rtype: list(str)\n",
    "            \"\"\"\n",
    "            token = tokens[idx]\n",
    "\n",
    "            feature_list = []\n",
    "\n",
    "            if not token:\n",
    "                return feature_list\n",
    "\n",
    "            # Capitalization\n",
    "            if token[0].isupper():\n",
    "                feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "            # Number\n",
    "            if re.search(self._pattern, token) is not None:\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "            # Punctuation\n",
    "            punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "            if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "            # Suffix up to length 3\n",
    "            if len(token) > 1:\n",
    "                feature_list.append(\"SUF_\" + token[-1:])\n",
    "            if len(token) > 2:\n",
    "                feature_list.append(\"SUF_\" + token[-2:])\n",
    "            if len(token) > 3:\n",
    "                feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "                \n",
    "            # Current word\n",
    "            feature_list.append(\"WORD_\" + token)\n",
    "            \n",
    "            ### WRITE YOUR OWN CODE HERE ###\n",
    "            if idx > 0:\n",
    "                feature_list.append(\"PREVWORD_\" + tokens[idx-1])\n",
    "            if idx < len(tokens)-1:\n",
    "                feature_list.append(\"NEXTWORD_\" + tokens[idx+1])\n",
    "                \n",
    "            ####\n",
    "\n",
    "            return feature_list\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3.3: Train your custom CRF tagger, then test it below. How does it compare to the default tagger? Why did adding the new features change the performance in this way? \n",
    "\n",
    "The results show how important it is understand your choice of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CustomCRF_NER_tagger(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CustomCRFTagger()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CustomCRF_NER_tagger(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.8228431071538226\n",
      "F1 score for class PER = 0.8226205191594561\n",
      "F1 score for class MISC = 0.7359635811836115\n",
      "F1 score for class ORG = 0.7015404364569962\n",
      "Macro-average f1 score = 0.7707419109884717\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL TODO 3.4: POS tags can be used as features for tasks like NER. Complete the code below to define another custom CRF tagger that also include POS tags as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Improve the CRF NER tagger using parts of speech (see lab 5) as additional features.\n",
    "class CRFTaggerWithPOS(CustomCRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, index):\n",
    "        \"\"\"\n",
    "        Extract the features for a token and append the POS tag as an additional feature.\n",
    "        \"\"\"\n",
    "        basic_features = super()._get_features(tokens, index)\n",
    "        \n",
    "        # Get the pos tags for the current sentence and save it\n",
    "        if tokens != self._current_tokens:\n",
    "            self._pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "            self._current_tokens = tokens\n",
    "            \n",
    "            \n",
    "        ### WRITE YOUR OWN CODE HERE\n",
    "        basic_features.append(self._pos_tagged_tokens[index][1])\n",
    "        ###\n",
    "        \n",
    "        return basic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CRF_NER_tagger_with_POS(train_set):\n",
    "    ### WRITE YOUR OWN CODE HERE\n",
    "    tagger = CRFTaggerWithPOS()\n",
    "    tagger.train(train_set, 'model.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CRF_NER_tagger_with_POS(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.8311450381679389\n",
      "F1 score for class PER = 0.8315085158150852\n",
      "F1 score for class MISC = 0.7500000000000001\n",
      "F1 score for class ORG = 0.7025657269559709\n",
      "Macro-average f1 score = 0.7788048202347487\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
